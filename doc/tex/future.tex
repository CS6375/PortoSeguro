
\documentclass{standalone}

\begin{document}

\section{Future Work}

Though multiple models were compared in our experiments, the overall performance was not very good.
Our final score received on Kaggle was 0.277 in Gini score, using the Gradient Boosting model of scikit-learn. The gold medal submission was 0.291. There was a great gap in between.

We believe our work can be further optimized in following aspects.

\subsubsection{Dimension Reduction}

In our preprocessing part, we calculated the correlation matrix to see the possible relations between features. As a result, we conclude that we can not ignore any feature justing by looking at the correlation matrix, since none of them are closely related. This decision can be made automatically using library.

Scikit-learn provide a \emph{Principal component analysis} (\verb|PCA|) library, performing linear dimensionality reduction using \emph{Singular Value Decomposition} of the data to project it to a lower dimensional space. We tried this library with a few experiments. Though the our result showed that there is still no clear separation between points, it is still a possible way to improve the final result.

Another method with great potential is t-Distributed Stochastic Neighbor Embedding (t-SNE)\cite{van2014accelerating}, a (prize-winning) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets\cite{tsne}. Unlike PCA, t-SNE preserves the local structure of data points.
The problem with scikit-learn implementation of t-SNE is its lack of memory optimization and running time is unbearably high. There are some optimized implementation in other language like Julia\cite{tsne:julia} and Java\cite{tsne:java}. Further works can try on these library for Dimension Reduction.

\end{document}